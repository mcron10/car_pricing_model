{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from warnings import filterwarnings \n",
    "filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Problem Definition:  \n",
    "\n",
    "The goal is to identify and model the relationships between used car prices and relevant predictor variables (also known as features) from historical sales data. This involves selecting, extracting, transforming, and analyzing a dataset containing information on used cars, including price, make, model, year, mileage, condition, trim level, location, and other relevant attributes, to develop a predictive model that captures the key drivers of used car prices. \n",
    "\n",
    "In this formulation: \n",
    "\n",
    "    Predictor variables  refer to the input features (e.g., make, model, year) that may influence used car prices.\n",
    "    Target variable  is the outcome or response we're interested in predicting (used car price).\n",
    "    Goal  is to develop a predictive model that can accurately estimate used car prices based on the predictor variables.\n",
    "     \n",
    "\n",
    "This reframed task aligns with the CRISP-DM methodology, which emphasizes a structured approach to data science projects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1 Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the data\n",
    "\n",
    "The following code will load the data into a pandas dataframe and then provide a highlevel view of the columns and column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_df = pd.read_csv('data/vehicles.csv', low_memory=False)\n",
    "vehicles_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing data\n",
    "The following code is used to count the number of rows in the DataFrame vehicles_df that contain at least one missing (NaN) value. Here's a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows = vehicles_df.isnull().T.any().T.sum()\n",
    "print('There are ' + str(nan_rows) + ' rows with at least one missing value.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove column and rows not needed and correct some manufacturer errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to process the vehicle dataset\n",
    "def process_vehicle_data(df):\n",
    "    # Handle missing values in 'cylinders' and 'drive'\n",
    "    df['cylinders'] = df['cylinders'].fillna('unknown')\n",
    "    df['drive'] = df['drive'].fillna('unknown')\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    columns_to_drop = ['VIN', 'size', 'Unnamed: 18']\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore').copy()\n",
    "\n",
    "    # Remove rows with missing essential data and zero price\n",
    "    essential_columns = ['price', 'year', 'manufacturer', 'model']\n",
    "    df.dropna(subset=essential_columns, inplace=True)\n",
    "    df = df[df['price'] != 0]\n",
    "\n",
    "    # Remove specific manufacturers\n",
    "    df = df[df['manufacturer'].str.lower() != 'harley-davidson']\n",
    "\n",
    "    # Replace specific manufacturer names\n",
    "    df['manufacturer'] = df['manufacturer'].replace({\n",
    "        'rover': 'land rover',\n",
    "        'mini': 'bmw'\n",
    "    })\n",
    "\n",
    "    # Clean and correct model names\n",
    "    df = clean_and_correct_model_names(df, redundant_words=[\n",
    "        '4x4', 'sedan', 'suv', 'coupe', 'hatchback', 'convertible', 'pickup', 'drw', 'benz', '4wd',\n",
    "        'wagon', 'cab', 'hd', 'crew', 'extended', 'utility', '2d', '4d', 'crew', 'sport', 'unlimited',\n",
    "        'connect', 'black', 'white', 'luxury', 'v6', 'all', 'new', 'lt', 'xlt', 'lx', 'xle', 'grand',\n",
    "        'limited', 'sr', 'big', 'horn', 'r/t', 'ltz', 'super', 'duty', 'ss', 'se', 'xl', 'gt', 'premium',\n",
    "        'st', 'ls', 'hard', 'top', '2.5i', 'regular', '2.5', 'le', 'exl', 'double', 'doub'\n",
    "    ])\n",
    "\n",
    "    # Encode categorical features\n",
    "    df = encode_categorical_features(df)\n",
    "\n",
    "    # Clean and fill cylinders and drive columns\n",
    "    df = clean_and_fill_cylinders_drive(df)\n",
    "\n",
    "    # Create vehicle categories\n",
    "    df = create_vehicle_categories(df, type_mapping={\n",
    "        'pickup': 'Truck/Van', 'truck': 'Truck/Van', 'van': 'Truck/Van', 'mini-van': 'Truck/Van',\n",
    "        'coupe': 'Car', 'sedan': 'Car', 'hatchback': 'Car', 'convertible': 'Car', 'wagon': 'Car',\n",
    "        'SUV': 'SUV/Offroad', 'offroad': 'SUV/Offroad', 'bus': 'Commercial/Other',\n",
    "        'other': 'Commercial/Other', 'unknown': 'Commercial/Other'\n",
    "    })\n",
    "\n",
    "    # Apply one-hot encoding\n",
    "    df = apply_one_hot_encoding(df, ['condition', 'title_status', 'vehicle_category', 'fuel', 'transmission'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to clean and correct model names\n",
    "def clean_and_correct_model_names(df, redundant_words):\n",
    "    # Convert the 'model' and 'manufacturer' columns to NumPy arrays for faster processing\n",
    "    model_array = df['model'].values.astype(str)\n",
    "    manufacturer_array = df['manufacturer'].values.astype(str)\n",
    "\n",
    "    # Convert manufacturer names to lowercase using np.char.lower\n",
    "    manufacturer_array = np.char.lower(manufacturer_array)\n",
    "\n",
    "    # Replace redundant words using a loop\n",
    "    for word in redundant_words:\n",
    "        model_array = np.char.replace(model_array, word, '')\n",
    "\n",
    "    # Apply the correction logic for Silverado 1500 models\n",
    "    chevy_mask = manufacturer_array == 'chevrolet'\n",
    "    silverado_mask = (np.char.find(np.char.lower(model_array), '1500') >= 0) | (np.char.find(np.char.lower(model_array), 'silverado') >= 0)\n",
    "\n",
    "    # Update the model names to 'Silverado 1500' where the conditions are met\n",
    "    model_array[chevy_mask & silverado_mask] = 'silverado 1500'\n",
    "\n",
    "    # Remove hyphens (\"-\") from the model names\n",
    "    model_array = np.char.replace(model_array, '-', '')\n",
    "\n",
    "    # Remove the substring \" x\", \" w\", \" s\" from the model names\n",
    "    model_array = np.char.replace(model_array, ' x', '')\n",
    "    model_array = np.char.replace(model_array, ' w', '')\n",
    "    model_array = np.char.replace(model_array, ' s', '')\n",
    "\n",
    "    # Strip leading and trailing spaces from the model names\n",
    "    model_array = np.char.strip(model_array)\n",
    "\n",
    "    # Assign the processed array back to the DataFrame\n",
    "    df['model_corrected'] = model_array\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to encode categorical features\n",
    "def encode_categorical_features(df):\n",
    "    # Frequency Encoding for 'manufacturer'\n",
    "    df['manufacturer_freq'] = df['manufacturer'].map(df['manufacturer'].value_counts())\n",
    "\n",
    "    # Frequency Encoding for 'model'\n",
    "    df['model_freq'] = df['model_corrected'].map(df['model_corrected'].value_counts())\n",
    "\n",
    "    # Frequency Encoding for 'region'\n",
    "    df['region_freq'] = df['region'].map(df['region'].value_counts())\n",
    "\n",
    "    # Frequency Encoding for 'state'\n",
    "    df['state_encoded'] = df['state'].map(df['state'].value_counts())\n",
    "\n",
    "    # Ordinal Encoding for 'paint_color'\n",
    "    color_mapping = {\n",
    "        'unknown': 0, 'white': 1, 'black': 2, 'silver': 3, 'blue': 4, 'red': 5, 'grey': 6,\n",
    "        'green': 7, 'custom': 8, 'brown': 9, 'yellow': 10, 'orange': 11, 'purple': 12\n",
    "    }\n",
    "    df['paint_color_ordinal'] = df['paint_color'].map(color_mapping)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to clean and fill cylinders and drive based on model\n",
    "def clean_and_fill_cylinders_drive(df):\n",
    "    # Create the cylinders_drive dictionary from non-missing data\n",
    "    cylinders_drive_df = df[(df['cylinders'] != 0) & (df['drive'] != 'unknown')].drop_duplicates(subset='model')\n",
    "    cylinders_drive = cylinders_drive_df.set_index('model')[['cylinders', 'drive']].to_dict('index')\n",
    "\n",
    "    def fill_cylinders(row):\n",
    "        # Check if the model exists in the dictionary\n",
    "        if row['cylinders'] == 0 and row['model'] in cylinders_drive:\n",
    "            return cylinders_drive[row['model']][0]\n",
    "        return row['cylinders']\n",
    "\n",
    "    def fill_drive(row):\n",
    "        # Check if the model exists in the dictionary\n",
    "        if row['drive'] == 'unknown' and row['model'] in cylinders_drive:\n",
    "            return cylinders_drive[row['model']][1]\n",
    "        return row['drive']\n",
    "\n",
    "    df['cylinders'] = df.apply(fill_cylinders, axis=1)\n",
    "    df['drive'] = df.apply(fill_drive, axis=1)\n",
    "\n",
    "    # Remove rows where 'cylinders' or 'drive' are still not determined\n",
    "    df = df[(df['cylinders'] != 0) & (df['drive'] != 'unknown')]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to create vehicle categories\n",
    "def create_vehicle_categories(df, type_mapping):\n",
    "    df['vehicle_category'] = df['type'].map(type_mapping)\n",
    "    df['vehicle_category'] = df['vehicle_category'].fillna('Commercial/Other')\n",
    "    return df\n",
    "\n",
    "# Function to apply one-hot encoding\n",
    "def apply_one_hot_encoding(df, columns_to_encode):\n",
    "    dummies = pd.get_dummies(df[columns_to_encode], prefix=columns_to_encode)\n",
    "    dummies = dummies.astype(int)\n",
    "    df = pd.concat([df.drop(columns=columns_to_encode), dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "vehicles_df_cleaned = process_vehicle_data(vehicles_df)\n",
    "\n",
    "# Convert 'year' to Datetime and Calculate 'age'\n",
    "vehicles_df_cleaned['year'] = pd.to_datetime(vehicles_df_cleaned['year'], format='%Y', errors='coerce')\n",
    "current_year = pd.to_datetime('today').year\n",
    "vehicles_df_cleaned['age'] = current_year - vehicles_df_cleaned['year'].dt.year\n",
    "\n",
    "# Convert 'odometer' to integers\n",
    "vehicles_df_cleaned['odometer'] = vehicles_df_cleaned['odometer'].astype(int)\n",
    "\n",
    "# Reset the index of the final DataFrame\n",
    "vehicles_df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the Final DataFrame\n",
    "print(\"\\nFinal DataFrame:\")\n",
    "print(vehicles_df_cleaned.head())\n",
    "\n",
    "# Save the final DataFrame to CSV\n",
    "vehicles_df_cleaned.to_csv('final_processed_vehicles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-numeric columns\n",
    "\n",
    "df_numeric = vehicles_df_final\n",
    "\n",
    "# Calculate the correlation matrix for the numeric columns\n",
    "correlation_matrix = df_numeric.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you proceed with machine learning, you need to ensure your data is properly prepared. This includes handling missing values, encoding categorical variables, scaling features, and applying logarithmic transformations where appropriate.\n",
    "\n",
    "Steps:\n",
    "\n",
    "    Handle Missing Values: Ensure that all missing values are addressed.\n",
    "    Encoding Categorical Variables: Use methods like one-hot encoding or label encoding, depending on the type of categorical variable.\n",
    "    Feature Scaling: Normalize or standardize numerical features to bring them onto a similar scale, especially for regression models.\n",
    "    Logarithmic Transformation: Apply log transformation to skewed numerical features to stabilize variance and make the data more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vehicles_df_final\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values:\\n\", missing_values)\n",
    "\n",
    "# Encoding categorical variables\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Log transform skewed numerical features\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: np.log1p(x) if np.abs(x.skew()) > 0.5 else x)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Verify data preparation\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models. Here, you should build a number of different regression models with the price as the target. In building your models, you should explore different parameters and be sure to cross-validate your findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of Multiple Regression Models\n",
    "\n",
    "Now that your data is prepared, you can apply multiple regression models to it. Here are a few models you could consider:\n",
    "\n",
    "    Linear Regression\n",
    "    Ridge Regression\n",
    "    Lasso Regression\n",
    "    ElasticNet Regression\n",
    "    Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop(columns=['price'])  # Assuming 'price' is the target variable\n",
    "y = df['price']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'ElasticNet Regression': ElasticNet(),\n",
    "#    'Random Forest': RandomForestRegressor()\n",
    "}\n",
    "\n",
    "# Evaluate each model using cross-validation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{name} - CV MSE: {-cv_scores.mean()} - Test MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation and Grid Search for Hyperparameters\n",
    "\n",
    "Use cross-validation to assess model performance and grid search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example: Grid search for Ridge Regression\n",
    "ridge = Ridge()\n",
    "param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0]}\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Ridge Regression:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", -grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Coefficients\n",
    "\n",
    "Once you have the best model, you can interpret the coefficients (for linear models) to understand the impact of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_ridge_model = Ridge(alpha=100.0)\n",
    "best_ridge_model.fit(X_train, y_train)\n",
    "y_pred = best_ridge_model.predict(X_test)\n",
    "\n",
    "# Calculate test MSE\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE for the best Ridge Regression model: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Model Coefficients:\n",
    "\n",
    " Since Ridge Regression is a linear model, you can interpret the coefficients to understand the impact of each feature on the target variable (price). This can provide insights into which features are most influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display coefficients\n",
    "feature_names = X_train.columns\n",
    "coefficients = best_ridge_model.coef_\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(coef_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=coef_df, palette='coolwarm')\n",
    "plt.title('Feature Coefficients')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming coef_df is your DataFrame with features and coefficients\n",
    "# and vehicles_df_cleaned (the preprocessed DataFrame before dropping original columns) is available\n",
    "\n",
    "# Select the top 10 features with the highest positive coefficients\n",
    "top_positive_features = coef_df.sort_values(by='Coefficient', ascending=False).head(10)['Feature']\n",
    "\n",
    "# Select the top 10 features with the lowest (most negative) coefficients\n",
    "top_negative_features = coef_df.sort_values(by='Coefficient', ascending=True).head(10)['Feature']\n",
    "\n",
    "# Original columns to include in the output\n",
    "original_columns = ['manufacturer', 'model', 'price', 'odometer', 'year']  # Add any other original columns you want to see\n",
    "\n",
    "# Ensure the DataFrame you're working with includes these original columns\n",
    "vehicles_df_with_originals = vehicles_df_cleaned.copy()\n",
    "\n",
    "# Extract cars with top positive coefficients, including original columns\n",
    "top_positive_cars = vehicles_df_with_originals[\n",
    "    (vehicles_df_final[top_positive_features] > 0).any(axis=1)\n",
    "][original_columns + top_positive_features.tolist()].head(10)\n",
    "\n",
    "# Extract cars with top negative coefficients, including original columns\n",
    "top_negative_cars = vehicles_df_with_originals[\n",
    "    (vehicles_df_final[top_negative_features] > 0).any(axis=1)\n",
    "][original_columns + top_negative_features.tolist()].head(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nTop 10 Cars with Best Coefficients:\")\n",
    "print(top_positive_cars)\n",
    "\n",
    "print(\"\\nTop 10 Cars with Worst Coefficients:\")\n",
    "print(top_negative_cars)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title(\"Distribution of Residuals\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title(\"Residuals vs Predicted\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Suppose X_train has two features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "\n",
    "# X_poly will now contain original features, squared terms, and interaction terms\n",
    "print(X_poly.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight on drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine tuning their inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
